{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oDCBJNpvtY42"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsLrnk8ZtY45"
   },
   "source": [
    "\n",
    "Language Translation with nn.Transformer and torchtext\n",
    "======================================================\n",
    "\n",
    "This tutorial shows:\n",
    "    - How to train a translation model from scratch using Transformer. \n",
    "    - Use tochtext library to access  `Multi30k <http://www.statmt.org/wmt16/multimodal-task.html#task1>`__ dataset to train a German to English translation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAXAHGaftY47"
   },
   "source": [
    "Data Sourcing and Processing\n",
    "----------------------------\n",
    "\n",
    "`torchtext library <https://pytorch.org/text/stable/>`__ has utilities for creating datasets that can be easily\n",
    "iterated through for the purposes of creating a language translation\n",
    "model. In this example, we show how to use torchtext's inbuilt datasets, \n",
    "tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n",
    "`Multi30k dataset from torchtext library <https://pytorch.org/text/stable/datasets.html#multi30k>`__\n",
    "that yields a pair of source-target raw sentences. \n",
    "\n",
    "To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchtext==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "xQHNleVgtY48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
      "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 30.1 MB/s eta 0:00:01█████████████████████████████▍  | 11.8 MB 30.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.23.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting de-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 670 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.21.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (57.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "!pip install -U torchdata\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    " \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator \n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylI_IroGtY49"
   },
   "source": [
    "Seq2Seq Network using Transformer\n",
    "---------------------------------\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in `“Attention is all you\n",
    "need” <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>`__\n",
    "paper for solving machine translation tasks. \n",
    "Below, we will create a Seq2Seq network that uses Transformer. The network\n",
    "consists of three parts. First part is the embedding layer. This layer converts tensor of input indices\n",
    "into corresponding tensor of input embeddings. These embedding are further augmented with positional\n",
    "encodings to provide position information of input tokens to the model. The second part is the \n",
    "actual `Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ model. \n",
    "Finally, the output of Transformer model is passed through linear layer\n",
    "that give un-normalized probabilities for each token in the target language. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0432, -0.0164,  0.0252,  ...,  0.0505,  0.0237, -0.0462],\n",
      "        [ 0.0208, -0.0073,  0.0032,  ...,  0.0018,  0.0075, -0.0527],\n",
      "        [-0.0218, -0.0374,  0.0386,  ..., -0.0535, -0.0170, -0.0449],\n",
      "        ...,\n",
      "        [ 0.0217,  0.0480,  0.0311,  ..., -0.0418,  0.0530,  0.0118],\n",
      "        [ 0.0120, -0.0095,  0.0197,  ...,  0.0405,  0.0143, -0.0124],\n",
      "        [-0.0492,  0.0264,  0.0027,  ...,  0.0290,  0.0370, -0.0538]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0745,  0.0376,  0.0762,  ...,  0.0123,  0.0109, -0.0531],\n",
      "        [-0.0555,  0.0182,  0.0381,  ...,  0.0220,  0.0467,  0.0121],\n",
      "        [-0.0262,  0.0160, -0.0466,  ..., -0.0546, -0.0713,  0.0553],\n",
      "        ...,\n",
      "        [ 0.0550, -0.0659, -0.0516,  ...,  0.0046, -0.0200,  0.0030],\n",
      "        [ 0.0023,  0.0259, -0.0720,  ...,  0.0709, -0.0646,  0.0004],\n",
      "        [-0.0056, -0.0642,  0.0124,  ..., -0.0330, -0.0051, -0.0183]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0181,  0.0412,  0.0477,  ..., -0.0064,  0.0208, -0.0274],\n",
      "        [-0.0183, -0.0246, -0.0218,  ..., -0.0023,  0.0122,  0.0156],\n",
      "        [-0.0220, -0.0048, -0.0031,  ..., -0.0316,  0.0291,  0.0215],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0062, -0.0333,  ..., -0.0124,  0.0344, -0.0313],\n",
      "        [-0.0213, -0.0112, -0.0315,  ..., -0.0239, -0.0441,  0.0394],\n",
      "        [ 0.0070,  0.0226,  0.0200,  ...,  0.0437, -0.0441,  0.0084]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0108,  0.0375, -0.0107,  ...,  0.0369, -0.0101, -0.0324],\n",
      "        [ 0.0463,  0.0206, -0.0052,  ...,  0.0389, -0.0350, -0.0011],\n",
      "        [-0.0181, -0.0244,  0.0025,  ..., -0.0112,  0.0458,  0.0035],\n",
      "        ...,\n",
      "        [ 0.0152, -0.0248,  0.0163,  ...,  0.0325, -0.0364, -0.0065],\n",
      "        [-0.0366,  0.0446,  0.0143,  ...,  0.0130,  0.0471,  0.0232],\n",
      "        [ 0.0071,  0.0068, -0.0049,  ..., -0.0133, -0.0315, -0.0473]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0225, -0.0205,  0.0160,  ..., -0.0484, -0.0228, -0.0383],\n",
      "        [ 0.0456, -0.0533,  0.0175,  ..., -0.0245,  0.0019, -0.0258],\n",
      "        [ 0.0372,  0.0082, -0.0211,  ...,  0.0343,  0.0119,  0.0144],\n",
      "        ...,\n",
      "        [ 0.0158, -0.0147, -0.0135,  ...,  0.0512, -0.0079,  0.0026],\n",
      "        [ 0.0293,  0.0500, -0.0116,  ...,  0.0086, -0.0475, -0.0427],\n",
      "        [ 0.0061, -0.0220,  0.0111,  ..., -0.0410, -0.0224, -0.0539]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0166, -0.0229, -0.0007,  ..., -0.0425,  0.0687, -0.0701],\n",
      "        [ 0.0030, -0.0460, -0.0264,  ..., -0.0397,  0.0033, -0.0512],\n",
      "        [ 0.0027, -0.0326,  0.0679,  ...,  0.0723,  0.0596,  0.0337],\n",
      "        ...,\n",
      "        [-0.0246,  0.0281,  0.0373,  ..., -0.0750, -0.0664,  0.0197],\n",
      "        [ 0.0023,  0.0139,  0.0254,  ...,  0.0649, -0.0488,  0.0429],\n",
      "        [ 0.0618,  0.0517, -0.0303,  ..., -0.0619, -0.0400, -0.0092]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0401, -0.0275, -0.0189,  ...,  0.0318, -0.0018, -0.0435],\n",
      "        [ 0.0434, -0.0447,  0.0217,  ...,  0.0267, -0.0309, -0.0390],\n",
      "        [ 0.0390, -0.0026, -0.0365,  ...,  0.0072,  0.0323,  0.0420],\n",
      "        ...,\n",
      "        [ 0.0458,  0.0258,  0.0011,  ..., -0.0426,  0.0294, -0.0255],\n",
      "        [-0.0124, -0.0347, -0.0156,  ..., -0.0187, -0.0111,  0.0163],\n",
      "        [-0.0087,  0.0351, -0.0203,  ..., -0.0240,  0.0023,  0.0398]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-3.7236e-02,  3.2353e-02,  4.6308e-02,  ..., -3.1721e-03,\n",
      "          9.6911e-04,  4.8534e-03],\n",
      "        [ 1.6545e-02,  2.7954e-03,  1.1537e-02,  ..., -9.7301e-03,\n",
      "         -2.7172e-02, -3.6523e-02],\n",
      "        [-5.6120e-03,  4.3161e-03, -1.2769e-02,  ...,  1.4329e-02,\n",
      "         -3.8510e-02, -1.6906e-02],\n",
      "        ...,\n",
      "        [-3.7480e-02, -8.3713e-03, -1.6926e-03,  ..., -3.9014e-02,\n",
      "          3.4156e-02, -2.7700e-03],\n",
      "        [ 4.8535e-03,  3.8026e-02,  5.0648e-05,  ..., -5.7489e-03,\n",
      "         -1.0683e-02,  3.5973e-02],\n",
      "        [-1.0926e-03, -1.9348e-02,  4.8094e-02,  ..., -5.6270e-03,\n",
      "          4.5936e-02,  3.2929e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0063, -0.0529,  0.0053,  ...,  0.0519, -0.0458, -0.0148],\n",
      "        [-0.0019, -0.0124,  0.0230,  ..., -0.0016,  0.0115,  0.0142],\n",
      "        [-0.0055, -0.0003,  0.0206,  ..., -0.0532, -0.0025, -0.0398],\n",
      "        ...,\n",
      "        [-0.0065,  0.0331,  0.0211,  ...,  0.0415,  0.0082,  0.0226],\n",
      "        [-0.0465, -0.0350, -0.0115,  ...,  0.0455, -0.0382,  0.0413],\n",
      "        [-0.0197,  0.0162,  0.0439,  ..., -0.0005,  0.0114, -0.0332]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0267, -0.0580,  0.0516,  ..., -0.0486, -0.0453,  0.0029],\n",
      "        [ 0.0472, -0.0379, -0.0439,  ...,  0.0304, -0.0164,  0.0704],\n",
      "        [-0.0500,  0.0609,  0.0752,  ..., -0.0760, -0.0231, -0.0622],\n",
      "        ...,\n",
      "        [ 0.0151,  0.0266,  0.0616,  ...,  0.0348,  0.0265, -0.0280],\n",
      "        [-0.0396, -0.0699,  0.0746,  ..., -0.0199,  0.0520,  0.0174],\n",
      "        [ 0.0490, -0.0510,  0.0277,  ..., -0.0567, -0.0610,  0.0434]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0345,  0.0464,  0.0161,  ...,  0.0206, -0.0437, -0.0266],\n",
      "        [-0.0444,  0.0405, -0.0209,  ...,  0.0129, -0.0214, -0.0049],\n",
      "        [ 0.0453,  0.0222,  0.0280,  ...,  0.0118,  0.0326, -0.0244],\n",
      "        ...,\n",
      "        [-0.0166,  0.0300,  0.0119,  ...,  0.0387,  0.0227, -0.0265],\n",
      "        [ 0.0124,  0.0090, -0.0046,  ...,  0.0128,  0.0141, -0.0461],\n",
      "        [ 0.0410,  0.0015, -0.0132,  ...,  0.0063, -0.0308, -0.0199]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0246, -0.0481,  0.0355,  ...,  0.0347,  0.0069,  0.0411],\n",
      "        [-0.0433, -0.0277,  0.0231,  ...,  0.0159,  0.0334,  0.0424],\n",
      "        [-0.0160, -0.0249, -0.0477,  ..., -0.0300,  0.0192, -0.0421],\n",
      "        ...,\n",
      "        [ 0.0458, -0.0194,  0.0102,  ...,  0.0461,  0.0421, -0.0378],\n",
      "        [-0.0012,  0.0384,  0.0182,  ...,  0.0035,  0.0282, -0.0373],\n",
      "        [ 0.0189,  0.0039,  0.0477,  ..., -0.0081,  0.0193,  0.0378]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0196, -0.0509,  0.0476,  ..., -0.0527, -0.0409, -0.0470],\n",
      "        [-0.0177,  0.0108,  0.0265,  ...,  0.0087,  0.0220, -0.0388],\n",
      "        [ 0.0160,  0.0038, -0.0489,  ..., -0.0214,  0.0135, -0.0339],\n",
      "        ...,\n",
      "        [ 0.0207,  0.0447,  0.0389,  ...,  0.0142,  0.0401, -0.0108],\n",
      "        [-0.0403,  0.0368, -0.0029,  ...,  0.0192, -0.0372,  0.0230],\n",
      "        [-0.0030, -0.0174,  0.0282,  ...,  0.0081, -0.0196,  0.0321]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0041,  0.0279,  0.0314,  ..., -0.0288,  0.0649,  0.0549],\n",
      "        [ 0.0362,  0.0740,  0.0148,  ...,  0.0231, -0.0342,  0.0441],\n",
      "        [ 0.0168,  0.0391,  0.0207,  ...,  0.0462, -0.0389, -0.0704],\n",
      "        ...,\n",
      "        [-0.0124, -0.0347,  0.0052,  ...,  0.0750,  0.0012, -0.0036],\n",
      "        [-0.0129,  0.0468, -0.0458,  ...,  0.0555, -0.0547, -0.0499],\n",
      "        [-0.0547, -0.0324, -0.0075,  ..., -0.0695,  0.0543,  0.0614]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0010,  0.0193, -0.0294,  ...,  0.0215,  0.0007, -0.0229],\n",
      "        [-0.0316, -0.0472,  0.0192,  ..., -0.0376,  0.0079, -0.0423],\n",
      "        [-0.0457, -0.0042,  0.0188,  ..., -0.0319, -0.0295,  0.0440],\n",
      "        ...,\n",
      "        [ 0.0352, -0.0277,  0.0337,  ...,  0.0121,  0.0005,  0.0411],\n",
      "        [ 0.0280, -0.0441, -0.0096,  ..., -0.0241,  0.0385, -0.0478],\n",
      "        [-0.0473, -0.0399, -0.0194,  ..., -0.0026,  0.0430,  0.0093]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0347, -0.0249, -0.0464,  ..., -0.0231, -0.0419,  0.0066],\n",
      "        [ 0.0403, -0.0414,  0.0239,  ...,  0.0194,  0.0199,  0.0031],\n",
      "        [ 0.0448, -0.0436,  0.0432,  ..., -0.0348,  0.0219, -0.0076],\n",
      "        ...,\n",
      "        [-0.0393,  0.0054,  0.0184,  ..., -0.0456, -0.0178,  0.0441],\n",
      "        [-0.0035, -0.0167,  0.0216,  ..., -0.0120, -0.0242,  0.0251],\n",
      "        [ 0.0251, -0.0363, -0.0361,  ..., -0.0319,  0.0335,  0.0010]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.5181e-02, -3.1691e-02, -1.5907e-03,  ...,  2.6946e-02,\n",
      "          5.3688e-02, -6.4547e-03],\n",
      "        [-1.6230e-02, -4.5023e-02, -2.9053e-02,  ...,  8.6976e-03,\n",
      "         -1.9492e-02,  3.7802e-02],\n",
      "        [-4.7749e-02,  2.7073e-02, -1.8631e-03,  ..., -4.2146e-02,\n",
      "         -2.1391e-02,  5.0195e-02],\n",
      "        ...,\n",
      "        [ 3.1287e-02, -2.1332e-02, -2.7648e-02,  ..., -3.2738e-02,\n",
      "          1.5621e-02, -3.8359e-02],\n",
      "        [ 2.3085e-02,  4.8838e-03, -4.7541e-02,  ...,  5.1882e-02,\n",
      "         -2.8786e-02,  1.6559e-02],\n",
      "        [-4.5367e-05,  3.7049e-03, -3.7140e-02,  ...,  3.8546e-02,\n",
      "         -3.2698e-03,  7.8010e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-5.1115e-02,  3.1834e-02,  4.5301e-02,  ...,  5.9511e-02,\n",
      "         -1.7328e-02,  8.8823e-05],\n",
      "        [-4.5753e-02, -3.1774e-02,  5.6860e-02,  ..., -5.5496e-02,\n",
      "         -1.0738e-02,  4.0071e-02],\n",
      "        [-7.0418e-02, -6.0622e-03,  5.1343e-02,  ..., -1.8159e-02,\n",
      "          2.9990e-02,  1.8447e-02],\n",
      "        ...,\n",
      "        [-4.2600e-02,  6.1255e-03, -3.2859e-02,  ...,  2.6726e-02,\n",
      "         -2.6290e-02, -1.2138e-02],\n",
      "        [ 5.7115e-02, -4.6572e-02, -5.1159e-02,  ..., -2.6202e-03,\n",
      "          6.4711e-02, -3.8294e-02],\n",
      "        [ 5.9920e-02, -7.5564e-03,  3.7865e-02,  ...,  3.5942e-02,\n",
      "          6.1518e-02, -6.1546e-03]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-4.4885e-02,  2.2698e-02,  2.2912e-03,  ..., -4.6069e-02,\n",
      "         -7.8130e-05, -4.0065e-02],\n",
      "        [-2.2600e-02, -1.5944e-02,  4.5397e-02,  ..., -4.7077e-03,\n",
      "          1.0075e-03,  1.5279e-02],\n",
      "        [-2.6763e-04,  3.0625e-02,  1.8565e-02,  ...,  3.2409e-02,\n",
      "          2.5012e-02, -3.9756e-02],\n",
      "        ...,\n",
      "        [-3.0000e-02,  1.0013e-02, -1.4896e-03,  ...,  8.7539e-03,\n",
      "          3.7172e-02, -1.7127e-02],\n",
      "        [-4.0145e-02, -2.9149e-02,  3.6508e-03,  ..., -5.8624e-03,\n",
      "         -1.3079e-02,  4.1074e-02],\n",
      "        [ 4.1570e-02, -4.7173e-02, -2.4419e-02,  ...,  2.7521e-02,\n",
      "         -4.3568e-02, -4.5219e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0395,  0.0308,  0.0009,  ...,  0.0130, -0.0370,  0.0074],\n",
      "        [-0.0092, -0.0253, -0.0387,  ...,  0.0472, -0.0165,  0.0093],\n",
      "        [ 0.0345, -0.0352,  0.0263,  ...,  0.0339, -0.0294, -0.0379],\n",
      "        ...,\n",
      "        [-0.0156, -0.0055,  0.0301,  ...,  0.0147,  0.0089, -0.0325],\n",
      "        [-0.0065, -0.0127,  0.0483,  ...,  0.0362, -0.0378,  0.0440],\n",
      "        [-0.0230, -0.0128,  0.0028,  ..., -0.0017,  0.0139, -0.0102]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0044, -0.0538,  0.0132,  ..., -0.0009,  0.0395,  0.0282],\n",
      "        [-0.0244,  0.0092,  0.0100,  ..., -0.0186, -0.0051,  0.0171],\n",
      "        [ 0.0420, -0.0392,  0.0419,  ..., -0.0174,  0.0008,  0.0285],\n",
      "        ...,\n",
      "        [ 0.0347, -0.0109,  0.0307,  ...,  0.0241, -0.0500,  0.0191],\n",
      "        [-0.0298, -0.0210,  0.0199,  ...,  0.0423, -0.0138, -0.0454],\n",
      "        [-0.0445, -0.0522,  0.0171,  ..., -0.0435, -0.0424, -0.0393]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0466, -0.0401, -0.0073,  ...,  0.0062,  0.0348, -0.0287],\n",
      "        [ 0.0110,  0.0262,  0.0137,  ...,  0.0398,  0.0244, -0.0438],\n",
      "        [ 0.0257,  0.0017, -0.0061,  ...,  0.0708, -0.0256, -0.0046],\n",
      "        ...,\n",
      "        [-0.0260, -0.0609, -0.0384,  ...,  0.0304, -0.0302,  0.0498],\n",
      "        [-0.0561, -0.0616,  0.0084,  ...,  0.0124, -0.0703,  0.0515],\n",
      "        [-0.0605,  0.0255, -0.0741,  ...,  0.0564, -0.0730,  0.0760]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0088, -0.0236,  0.0292,  ..., -0.0341, -0.0196,  0.0478],\n",
      "        [ 0.0332, -0.0467, -0.0462,  ..., -0.0304,  0.0097,  0.0434],\n",
      "        [-0.0243,  0.0084, -0.0455,  ...,  0.0002, -0.0056, -0.0153],\n",
      "        ...,\n",
      "        [-0.0366,  0.0153, -0.0420,  ...,  0.0146, -0.0257,  0.0021],\n",
      "        [-0.0251,  0.0104, -0.0233,  ..., -0.0177, -0.0196,  0.0451],\n",
      "        [-0.0117, -0.0350,  0.0040,  ...,  0.0301,  0.0253, -0.0030]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0426, -0.0376,  0.0036,  ...,  0.0018,  0.0434,  0.0046],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0161, -0.0466, -0.0436,  ...,  0.0036,  0.0306,  0.0347],\n",
      "        [ 0.0285, -0.0004, -0.0193,  ..., -0.0151,  0.0308,  0.0429],\n",
      "        [ 0.0288, -0.0381, -0.0215,  ...,  0.0149,  0.0480, -0.0009],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0298, -0.0079,  ..., -0.0340, -0.0306, -0.0360],\n",
      "        [-0.0233,  0.0292, -0.0401,  ..., -0.0220,  0.0034,  0.0135],\n",
      "        [ 0.0225, -0.0070,  0.0456,  ..., -0.0173, -0.0011, -0.0409]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.7554e-02, -1.0969e-02,  3.0408e-03, -2.1540e-02, -4.8940e-03,\n",
      "         6.1876e-03, -1.3217e-04, -4.6238e-03, -1.8129e-02, -1.5327e-02,\n",
      "        -1.5447e-02, -8.5916e-03,  1.9227e-02,  2.0760e-02, -9.6496e-03,\n",
      "         5.9363e-03,  1.3873e-02, -1.6751e-02, -2.1757e-02,  3.0419e-04,\n",
      "         1.1954e-02, -2.5335e-03, -8.3260e-03,  1.5559e-02,  1.9236e-02,\n",
      "         1.3244e-02, -2.1278e-02,  1.5483e-02,  1.1591e-02, -8.4414e-03,\n",
      "        -1.0609e-02,  1.9342e-02, -3.9002e-03, -1.8238e-02, -1.7617e-02,\n",
      "        -1.9562e-02,  1.7361e-02,  2.0565e-02, -1.9947e-03, -2.4680e-03,\n",
      "        -1.9302e-02, -3.5298e-03,  5.3759e-03,  2.1507e-02, -8.8921e-03,\n",
      "        -1.6644e-02, -1.2138e-02, -1.9009e-02,  1.0711e-02, -1.3562e-02,\n",
      "         1.6520e-02,  7.0582e-03,  8.3604e-03, -1.7976e-02, -1.4084e-02,\n",
      "        -2.0584e-02, -1.0346e-03,  5.1173e-04,  2.2084e-02, -4.8588e-03,\n",
      "         3.6785e-03, -2.9970e-03,  8.6950e-03, -2.1103e-03,  1.5652e-02,\n",
      "        -1.6809e-02,  1.0025e-02, -9.0054e-03,  8.0651e-03,  2.0681e-02,\n",
      "         2.1643e-02, -3.0282e-03,  9.5596e-03,  1.6918e-02,  2.1061e-02,\n",
      "         5.7713e-04,  3.3459e-03,  1.8507e-02, -1.8830e-02, -1.9834e-02,\n",
      "        -6.6671e-03,  1.2110e-02, -4.2527e-03, -1.6087e-02,  2.0580e-02,\n",
      "        -9.5056e-03, -1.5160e-02,  1.4308e-02, -1.2246e-02, -1.5620e-02,\n",
      "        -1.5824e-02,  1.6667e-02,  1.8099e-02,  2.0141e-02, -2.1884e-02,\n",
      "         1.6204e-02, -3.3046e-03, -1.5879e-02, -7.4466e-03, -9.4885e-03,\n",
      "         1.3535e-02, -1.5060e-03,  5.2426e-03,  5.6364e-03,  5.6967e-03,\n",
      "         3.6446e-03,  1.5046e-04, -1.4691e-02, -1.6302e-02, -1.9365e-02,\n",
      "         1.1973e-02,  2.0528e-02,  9.5683e-03, -2.2872e-03,  1.5241e-02,\n",
      "         1.6348e-02,  2.0562e-02, -1.5140e-02,  8.2981e-03, -1.0832e-02,\n",
      "         1.1629e-02, -3.9687e-04, -6.4629e-03,  1.6197e-02,  4.5787e-04,\n",
      "         1.9292e-02,  1.4803e-02, -1.5029e-02, -5.2164e-03,  1.2197e-03,\n",
      "        -1.2982e-02, -7.5195e-04, -1.8027e-02,  1.7000e-03,  4.0247e-03,\n",
      "        -3.7539e-03,  1.4099e-02,  4.9721e-03,  1.6496e-02, -2.8245e-03,\n",
      "         2.5656e-03, -8.6859e-03,  7.5399e-03,  1.9491e-02, -1.5636e-02,\n",
      "         2.1199e-02, -1.8076e-02, -1.3155e-02,  9.8725e-03,  6.0175e-03,\n",
      "         1.3915e-02,  4.0470e-03, -1.3416e-02,  1.0048e-02,  3.2527e-03,\n",
      "         9.5998e-03, -5.6604e-03, -1.1285e-02,  2.0664e-02,  1.0804e-02,\n",
      "         1.2475e-02,  9.8864e-03, -1.1258e-02, -1.4620e-03,  1.2180e-02,\n",
      "         1.1783e-03, -1.9763e-03,  3.2883e-03,  1.8861e-02,  1.3670e-02,\n",
      "         2.0697e-02,  1.4633e-02,  8.6963e-03, -2.5709e-03,  1.8787e-02,\n",
      "         2.0171e-02, -1.1837e-02,  2.1510e-02, -1.1600e-02, -3.4243e-03,\n",
      "         1.6967e-03, -5.5891e-03,  3.2971e-03,  1.2091e-03,  6.5447e-03,\n",
      "        -1.1628e-02, -1.5730e-02, -2.0272e-02,  6.4184e-03, -6.8037e-04,\n",
      "        -8.7705e-03,  5.7155e-03, -1.1815e-03, -1.1315e-02,  1.5439e-02,\n",
      "         1.9852e-02, -1.9157e-02, -1.3795e-02,  7.1515e-03,  1.6366e-02,\n",
      "         8.9157e-03,  1.9523e-02, -1.7149e-02, -1.4789e-02, -2.0629e-02,\n",
      "        -4.9514e-03, -6.6200e-04, -8.1961e-03,  6.1554e-03,  1.7424e-02,\n",
      "        -1.6551e-02,  1.0276e-03,  2.0704e-02,  2.7978e-03, -4.0514e-03,\n",
      "        -1.3684e-02, -1.9912e-02,  1.8316e-02, -1.3076e-02,  2.0552e-02,\n",
      "        -3.2926e-03,  1.8724e-02,  3.8378e-03, -1.1606e-02, -7.1745e-03,\n",
      "        -6.6785e-03, -7.8203e-03, -2.0545e-02,  1.9942e-02, -6.3407e-03,\n",
      "         1.1132e-03,  2.1498e-02, -1.2786e-02,  1.4519e-02,  1.9859e-02,\n",
      "        -5.5778e-03,  1.6829e-02,  6.6510e-03, -2.2639e-03, -1.0325e-02,\n",
      "         1.4162e-02, -4.5103e-03,  1.6315e-02, -1.8295e-02,  1.6416e-02,\n",
      "         3.4632e-03, -1.4141e-02,  1.7403e-02,  1.2413e-02, -2.0539e-02,\n",
      "         7.6715e-03,  6.8739e-04,  2.0930e-02,  4.3259e-03,  3.7212e-03,\n",
      "        -1.5592e-02, -3.0059e-03, -1.0882e-02,  1.1542e-02, -1.5552e-02,\n",
      "        -1.6424e-02, -1.8764e-02, -1.4855e-02, -5.0853e-03,  5.2399e-04,\n",
      "         2.9706e-03, -7.6572e-03,  1.5889e-02,  6.9897e-03, -7.4765e-03,\n",
      "        -5.1638e-03, -3.6581e-03,  2.2003e-02,  1.8815e-02,  1.2344e-02,\n",
      "        -2.4628e-03,  1.9913e-02, -8.2092e-04,  7.4087e-03, -1.0691e-02,\n",
      "        -7.7000e-03,  3.3823e-03,  1.0755e-02,  2.1100e-02, -1.9418e-02,\n",
      "        -1.6287e-02,  3.4313e-03, -2.0418e-03, -2.1176e-02,  1.7872e-02,\n",
      "        -1.9016e-02,  1.6483e-02,  1.8707e-02, -9.6484e-03, -2.7307e-03,\n",
      "        -1.3322e-02,  1.4719e-02, -3.3237e-03,  1.0609e-02, -1.4874e-02,\n",
      "        -1.7060e-02,  2.0745e-02,  1.4690e-02,  1.0694e-02, -1.9068e-02,\n",
      "         2.0178e-02,  2.1814e-03, -8.6338e-03, -5.5383e-03,  1.0833e-02,\n",
      "         4.7580e-03, -1.1059e-02,  1.1065e-02,  1.0096e-02, -1.3942e-02,\n",
      "        -1.9924e-02, -1.8235e-02, -1.8066e-02,  9.0528e-03, -1.8728e-02,\n",
      "        -2.1716e-02, -1.1303e-02,  4.0789e-03,  1.3163e-02,  3.7944e-03,\n",
      "         2.7665e-04,  1.4034e-03,  4.2806e-03,  2.6658e-03, -6.2599e-03,\n",
      "         1.7722e-02,  2.4141e-03,  3.4095e-03,  2.6263e-06, -3.5114e-03,\n",
      "         1.2486e-02, -1.5972e-02,  3.1788e-03,  1.3918e-02,  9.1644e-03,\n",
      "         1.1127e-04, -1.3284e-02,  7.6582e-03,  9.6197e-03, -4.3066e-04,\n",
      "        -1.8527e-02, -4.2072e-03, -2.1756e-02, -6.5150e-03,  3.7565e-03,\n",
      "        -7.4120e-03,  1.6987e-02,  1.7193e-02, -1.1309e-02,  1.0597e-02,\n",
      "        -7.1942e-03, -8.4531e-03,  1.9904e-02, -1.5251e-02,  3.1392e-03,\n",
      "        -2.1317e-02,  1.3237e-02,  3.6510e-03,  1.5174e-02, -2.1286e-04,\n",
      "        -2.0719e-02, -1.9379e-02, -1.1782e-02, -1.6942e-02, -1.6054e-02,\n",
      "         1.0513e-02, -1.9512e-02,  1.3788e-03,  1.7813e-02,  2.4349e-03,\n",
      "         2.2018e-02, -1.4645e-02,  1.4777e-02,  1.7297e-02, -6.0534e-03,\n",
      "         3.1725e-03,  1.9015e-02, -1.8285e-02,  1.3892e-02,  1.6403e-02,\n",
      "         1.8998e-02, -1.8431e-02,  7.1955e-03, -6.5604e-04, -1.3845e-02,\n",
      "        -7.6566e-04,  1.7803e-02, -1.9252e-02,  1.2000e-02,  1.9367e-02,\n",
      "         9.7751e-03,  2.5150e-03, -1.4466e-02,  1.8711e-02,  6.6539e-03,\n",
      "         1.9373e-02,  1.5823e-02, -8.6268e-03,  1.8679e-02,  1.3750e-02,\n",
      "        -1.5136e-02, -8.1623e-03,  1.3878e-02,  1.1793e-02, -1.3890e-02,\n",
      "        -3.4123e-03,  1.2195e-02, -1.4879e-02,  4.2311e-03, -9.9605e-03,\n",
      "        -2.1529e-02, -3.1591e-03,  1.5116e-02,  1.6373e-02,  2.1059e-02,\n",
      "        -2.0269e-02,  1.3833e-02,  1.9932e-02,  1.3780e-02,  9.0128e-03,\n",
      "        -7.7774e-03, -5.7280e-03,  3.7854e-03,  1.7429e-02, -8.7698e-03,\n",
      "        -1.1066e-02,  1.1676e-02,  8.2438e-03,  1.0057e-02,  9.9951e-03,\n",
      "        -2.0014e-02, -1.5676e-02, -3.6617e-04, -1.5953e-02, -8.9693e-03,\n",
      "        -1.3578e-02,  1.6315e-02, -1.9798e-02,  2.1747e-03, -6.4602e-03,\n",
      "         1.0128e-02,  1.9653e-03,  2.0281e-02, -1.9635e-02, -2.0655e-02,\n",
      "        -2.5740e-03, -3.6898e-03,  7.9947e-03, -7.4924e-04,  1.6869e-02,\n",
      "        -2.2894e-03, -9.1171e-03,  5.1919e-03,  1.1219e-02, -7.9369e-03,\n",
      "        -9.4569e-03,  8.5503e-03, -8.8271e-03,  7.1676e-04,  1.0118e-02,\n",
      "         1.9658e-03,  1.2698e-02, -2.0858e-03,  1.6684e-02,  1.8981e-03,\n",
      "        -7.1947e-03,  2.0159e-02, -2.0089e-03, -1.2032e-02,  1.2225e-02,\n",
      "        -1.6778e-02, -1.6523e-02,  8.3380e-03,  4.6890e-03, -1.5417e-02,\n",
      "        -1.6397e-02,  8.8290e-04, -7.1991e-03,  1.8388e-02, -4.2915e-03,\n",
      "        -2.1140e-02,  3.4607e-03, -1.3082e-02, -2.1071e-02, -1.2877e-02,\n",
      "         1.0654e-02, -1.9264e-02,  5.1692e-03, -2.5268e-03,  2.0277e-02,\n",
      "         1.6755e-02,  1.3020e-03, -2.0882e-02, -1.1087e-02,  2.1049e-02,\n",
      "        -2.6818e-03, -1.0748e-02,  1.1510e-02, -1.4330e-02, -7.2085e-03,\n",
      "        -8.5618e-03, -2.1254e-02, -3.5385e-03,  2.0992e-02, -1.6176e-02,\n",
      "        -1.9572e-02, -2.1651e-02], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for j in i.parameters():\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'enable_nested_tensor',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'layers',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm',\n",
       " 'num_layers',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerEncoder' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-48a1c4b39f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransformer_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerEncoder' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for i in transformer_layer.children():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_reset_parameters',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'batch_first',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'd_model',\n",
       " 'decoder',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'encoder',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'generate_square_subsequent_mask',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'nhead',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transformer_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u7qEVe_NtY4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WumuNKwUtY4-"
   },
   "source": [
    "During training, we need a subsequent word mask that will prevent model to look into\n",
    "the future words when making predictions. We will also need masks to hide\n",
    "source and target padding tokens. Below, let's define a function that will take care of both. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1SFD0JntY4_"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZMtb-D0tY5A"
   },
   "source": [
    "Let's now define the parameters of our model and instantiate the same. Below, we also \n",
    "define our loss function which is the cross-entropy loss and the optmizer used for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYsTRPNFtY5A"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afu-0JDQtY5B"
   },
   "source": [
    "Collation\n",
    "---------\n",
    "\n",
    "As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
    "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
    "defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
    "can be fed directly into our model.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU0cDJxxtY5B"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_5pEQQMtY5C"
   },
   "source": [
    "Let's define training and evaluation loop that will be called for each \n",
    "epoch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9tv-DxJtY5C"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    \n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt91rlHvtY5D"
   },
   "source": [
    "Now we have all the ingredients to train our model. Let's do it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8mTsSK6tY5D"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm \n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Msx9OBIFtY5D"
   },
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y88yu2HItY5E"
   },
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "1. Attention is all you need paper.\n",
    "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Копия блокнота \"translation_transformer.ipynb\"",
   "provenance": [
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/8cdd9a659f7d22e15eb4a689206e4b6b/translation_transformer.ipynb",
     "timestamp": 1657131080336
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
